{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import *\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.2367],\n",
      "        [ 9.3307],\n",
      "        [ 6.9748],\n",
      "        ...,\n",
      "        [11.5261],\n",
      "        [ 9.3131],\n",
      "        [ 7.4145]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# create a tensor with random numbers between 0 and 1 of size (33945, 1)\n",
    "# x = 0.5 + torch.rand(33945, 1)\n",
    "seed = 28\n",
    "np.random.seed(seed)\n",
    "sample_functions = {False: np.random.uniform, True: np.random.randint}\n",
    "# sample uniformly from the range (discrete)\n",
    "discrete = False\n",
    "# this_range = [4, 7]\n",
    "this_range = [6.3, 11.7]\n",
    "this_sample_function = sample_functions[discrete]\n",
    "x = torch.tensor(this_sample_function(*this_range, 33945)).unsqueeze(1)\n",
    "# x = torch.tensor(this_sample_function(*this_range, 33945)).unsqueeze(1)\n",
    "# x = 0.5 + torch.rand(33945, 1)\n",
    "print(x)\n",
    "# torch.save(x, 'data_files/favorita/random_lead_times.pt')\n",
    "torch.save(x, 'data_files/favorita/random_underage_costs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a tensor .pt file\n",
    "df = pd.read_csv('data_files/favorita//tensors_row_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store_nbr  item_nbr     family  class  perishable\n",
      "0          1     99197  GROCERY I   1067           0\n",
      "1          1    103520  GROCERY I   1028           0\n",
      "2          1    108634  GROCERY I   1075           0\n",
      "3          1    108797  GROCERY I   1004           0\n",
      "4          1    108862  GROCERY I   1062           0\n",
      "33945\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_data = pd.read_csv(f'data_files/favorita/dates_with_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas = torch.tensor(common_data['days_from_christmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   7,   14,   21,   28,   35,   42,   49,   56,   63,   70,   77,   84,\n",
      "          91,   98,  105,  112,  119,  126,  133,  140,  147,  154,  161,  168,\n",
      "         175,  182, -176, -169, -162, -155, -148, -141, -134, -127, -120, -113,\n",
      "        -106,  -99,  -92,  -85,  -78,  -71,  -64,  -57,  -50,  -43,  -36,  -29,\n",
      "         -22,  -15,   -8,   -1,    6,   13,   20,   27,   34,   41,   48,   55,\n",
      "          62,   69,   76,   83,   90,   97,  104,  111,  118,  125,  132,  139,\n",
      "         146,  153,  160,  167,  174,  181, -177, -170, -163, -156, -149, -142,\n",
      "        -135, -128, -121, -114, -107, -100,  -93,  -86,  -79,  -72,  -65,  -58,\n",
      "         -51,  -44,  -37,  -30,  -23,  -16,   -9,   -2,    5,   12,   19,   26,\n",
      "          33,   40,   47,   54,   61,   68,   75,   82,   89,   96,  103,  110,\n",
      "         117,  124,  131,  138,  145,  152,  159,  166,  173,  180, -178, -171,\n",
      "        -164, -157, -150, -143, -136, -129, -122, -115, -108, -101,  -94,  -87,\n",
      "         -80,  -73,  -66,  -59,  -52,  -45,  -38,  -31,  -24,  -17,  -10,   -3,\n",
      "           4,   11,   18,   25,   32,   39,   46,   53,   60,   67,   74,   81,\n",
      "          88,   95,  102,  109,  116,  123,  130,  137,  144,  151,  158,  165,\n",
      "         172,  179, -180, -173, -166, -159, -152, -145, -138, -131, -124, -117,\n",
      "        -110, -103,  -96,  -89,  -82,  -75,  -68,  -61,  -54,  -47,  -40,  -33,\n",
      "         -26,  -19,  -12,   -5,    2,    9,   16,   23,   30,   37,   44,   51,\n",
      "          58,   65,   72,   79,   86,   93,  100,  107,  114,  121,  128,  135,\n",
      "         142,  149,  156,  163,  170,  177, -181, -174, -167, -160, -153, -146,\n",
      "        -139, -132])\n"
     ]
    }
   ],
   "source": [
    "print(christmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_files/one_store_backlogged.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "keys = 'seeds', 'shifted_seeds', 'problem_params', 'params_by_dataset', 'trainer_params', 'observation_params', 'store_params', 'warehouse_params', 'optimizer_params', 'nn_params'\n",
    "seeds, shifted_seeds, problem_params, params_by_dataset, trainer_params, observation_params, store_params, warehouse_params, optimizer_params, nn_params = [config[key] for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seeds', 'shifted_seeds', 'problem_params', 'params_by_dataset', 'observation_params', 'store_params', 'warehouse_params', 'echelon_params', 'trainer_params', 'optimizer_params', 'nn_params'])\n",
      "None\n",
      "{'sample_across_stores': False, 'vary_across_samples': False, 'expand': True, 'value': 4}\n"
     ]
    }
   ],
   "source": [
    "print(config.keys())\n",
    "print(f'{config[\"echelon_params\"]}')\n",
    "print(f'{config[\"store_params\"][\"lead_time\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ma4177/.conda/envs/MatiasRL/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "creator = DatasetCreator()\n",
    "\n",
    "scenario = Scenario(\n",
    "    params_by_dataset['train']['periods'], problem_params, store_params, warehouse_params, \n",
    "    params_by_dataset['train']['n_samples'] + params_by_dataset['dev']['n_samples'], seeds\n",
    "    )\n",
    "\n",
    "train_dataset, dev_dataset = creator.create_datasets(scenario, split=True, by_sample_indexes=True, sample_index_for_split=params_by_dataset['dev']['n_samples'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=params_by_dataset['train']['batch_size'], shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=params_by_dataset['dev']['batch_size'], shuffle=False)\n",
    "\n",
    "scenario = Scenario(params_by_dataset['test']['periods'], problem_params, store_params, warehouse_params, params_by_dataset['test']['n_samples'], shifted_seeds)\n",
    "test_dataset = creator.create_datasets(scenario, split=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params_by_dataset['test']['batch_size'], shuffle=False)\n",
    "\n",
    "data_loaders = {'train': train_loader, 'dev': dev_loader, 'test': test_loader}\n",
    "\n",
    "model = get_architecture(nn_params['name'])(nn_params, output_size=problem_params['n_stores'], device=device).to(device)\n",
    "# model = FullyConnectedNN(nn_params, output_size=problem_params['n_stores']).to(device)\n",
    "loss_function = PolicyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_params['learning_rate'])\n",
    "\n",
    "simulator = Simulator(device=device)\n",
    "trainer = Trainer(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RLDL/Code_to_submit/trainer.py:32\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mTrain the model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs): \u001b[38;5;66;03m# make multiple passes through the dataset\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     average_train_loss, average_train_loss_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperiods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_periods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_periods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_train_losses\u001b[38;5;241m.\u001b[39mappend(average_train_loss_to_report)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m trainer_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_dev_every_n_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/RLDL/Code_to_submit/trainer.py:111\u001b[0m, in \u001b[0;36mTrainer.do_one_epoch\u001b[0;34m(self, optimizer, data_loader, loss_function, simulator, model, periods, problem_params, observation_params, train, ignore_periods)\u001b[0m\n\u001b[1;32m    108\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m total_reward, reward_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_periods\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_reward\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# rewards from period 0\u001b[39;00m\n\u001b[1;32m    115\u001b[0m epoch_loss_to_report \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward_to_report\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# rewards from period ignore_periods onwards\u001b[39;00m\n",
      "File \u001b[0;32m~/RLDL/Code_to_submit/trainer.py:142\u001b[0m, in \u001b[0;36mTrainer.simulate_batch\u001b[0;34m(self, loss_function, simulator, model, periods, problem_params, data_batch, observation_params, ignore_periods)\u001b[0m\n\u001b[1;32m    135\u001b[0m observation, _ \u001b[38;5;241m=\u001b[39m simulator\u001b[38;5;241m.\u001b[39mreset(periods, problem_params, data_batch, observation_params)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(periods):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# print(f't: {t}')\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# print(f'store_inventories: {observation[\"store_inventories\"].shape}')\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# print()\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# print(f'observation.keys(): {observation.keys()}')\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# action['stores'] = action['stores'].round()\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# print(f'action: {action[\"\"][0]}')\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# make a deepcopy of the past observation\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     past_observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/MatiasRL/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/RLDL/Code_to_submit/neural_networks.py:87\u001b[0m, in \u001b[0;36mBaseStock.forward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))  \u001b[38;5;66;03m# constant base stock level\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstores\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minv_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(trainer_params['epochs'], loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_test_loss, average_test_loss_to_report = trainer.test(\n",
    "    loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, discrete_demand=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 5.658812561035156\n"
     ]
    }
   ],
   "source": [
    "print(f'test loss: {average_test_loss_to_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "ymin, ymax = 0, 100  # set the y-axis limit for the plot\n",
    "trainer.plot_losses(ymin=ymin, ymax=ymax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatiasRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
