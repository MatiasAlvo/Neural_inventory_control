{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from shared_imports.ipynb\n",
      "importing Jupyter notebook from data_handling.ipynb\n",
      "importing Jupyter notebook from neural_networks.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from shared_imports import *\n",
    "from data_handling import *\n",
    "from neural_networks import *\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator(gym.Env):\n",
    "    \"\"\"\n",
    "    Simulator class\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": None}\n",
    "\n",
    "    def __init__(self, device='cpu'):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            parameters: dict\n",
    "                dictionary containing the parameters of the environment\n",
    "        \"\"\"        \n",
    "        self.device = device\n",
    "        self.problem_params, self.observation_params = None, None\n",
    "        self.batch_size, self.n_stores, self.periods, self.observation, self._internal_data = None, None, None, None, None\n",
    "\n",
    "        # place_holders, will be overrided in reset method (as observation and action spaces depend on batch size, which might change during execution)\n",
    "        self.action_space = spaces.Dict({'stores': spaces.Box(low=0.0, high=np.inf, shape=(1, 1), dtype=np.float32)})\n",
    "        self.observation_space = spaces.Dict({'stores': spaces.Box(low=0.0, high=np.inf, shape=(1, 1), dtype=np.float32)})\n",
    "    \n",
    "    def reset(self, periods, problem_params,  data, observation_params):\n",
    "        \"\"\"\n",
    "        Reset the environment, including initializing the observation, and return first observation\n",
    "        \"\"\"\n",
    "\n",
    "        self.problem_params = problem_params\n",
    "        self.observation_params = observation_params\n",
    "\n",
    "        self.batch_size, self.n_stores, self.periods = len(data['initial_inventories']), problem_params['n_stores'], periods\n",
    "        \n",
    "\n",
    "        # data that can only be used by the simulator . E.g., all demands, days to christmas (including future)...\n",
    "        self._internal_data = {\n",
    "            'demands': data['demands'],\n",
    "            'period_shift': observation_params['demand']['period_shift'],\n",
    "            }\n",
    "        \n",
    "        if observation_params['include_days_to_christmas']:\n",
    "            self._internal_data['days_to_christmas'] = data['days_to_christmas']\n",
    "        self._internal_data['allocation_shift'] = self.initialize_shifts_for_allocation_put(data['initial_inventories'].shape).long().to(self.device)\n",
    "        if self.problem_params['n_warehouses'] > 0:\n",
    "            self._internal_data['warehouse_allocation_shift'] = self.initialize_shifts_for_allocation_put(data['initial_warehouse_inventories'].shape).long().to(self.device)\n",
    "        self._internal_data['zero_allocation_tensor'] = self.initialize_zero_allocation_tensor(data['initial_inventories'].shape[: -1]).to(self.device)\n",
    "\n",
    "        self.observation = self.initialize_observation(data, observation_params)\n",
    "        self.action_space = self.initialize_action_space(self.batch_size, problem_params, observation_params)\n",
    "        self.observation_space = self.initialize_observation_space(self.observation, periods, problem_params,)\n",
    "        \n",
    "        return self.observation, None\n",
    "    \n",
    "    def initialize_shifts_for_allocation_put(self, shape):\n",
    "        \"\"\"\n",
    "        We will add store's allocations into corresponding position by flatenning out the state vector of the\n",
    "        entire batch. We create allocation_shifts to calculate in which position of that long vector we have\n",
    "        to add the corresponding allocation\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, n_stores, lead_time_max = shape\n",
    "\n",
    "        # results in a vector of lenght batch_size, where each entry corresponds to the first position of an element of a given sample\n",
    "        # in the long vector of the entire batch\n",
    "        n_instance_store_shift = (\n",
    "            torch.arange(batch_size) * (lead_time_max * n_stores)\n",
    "            ).to(self.device)\n",
    "\n",
    "        # results in a tensor of shape batch_size x stores, where each entry corresponds to the number of positions, to move 'to the right'\n",
    "        # for each store, beginning from the first position within a sample\n",
    "        store_n_shift = (\n",
    "            torch.arange(n_stores) * (lead_time_max)\n",
    "            ).expand(batch_size, n_stores).to(self.device)\n",
    "        \n",
    "        # results in a vector of shape batch_size x stores, where each entry corresponds to the first position of an element of a given (sample, store)\n",
    "        # in the long vector of the entire batch\n",
    "        # we then add the corresponding lead time to obtain the actual position in which to insert the action\n",
    "        return n_instance_store_shift[:, None] + store_n_shift\n",
    "\n",
    "    def initialize_zero_allocation_tensor(self, shape):\n",
    "        \"\"\"\n",
    "        Initialize a tensor of zeros with the same shape as the allocation tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.zeros(shape).to(self.device)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Simulate one step in the environment, returning the new observation and the reward (per sample)\n",
    "        \"\"\"\n",
    "\n",
    "        current_demands = self.get_current_demands(\n",
    "            self._internal_data, \n",
    "            current_period=self.observation['current_period'].item()\n",
    "            )\n",
    "\n",
    "        # calculate reward and update store inventories\n",
    "        reward = self.calculate_store_reward_and_update_store_inventories(\n",
    "            current_demands,\n",
    "            action,\n",
    "            self.observation\n",
    "            )\n",
    "\n",
    "        # update current period\n",
    "        self.observation['current_period'] += 1\n",
    "\n",
    "        terminated = self.observation['current_period'] >= self.periods\n",
    "\n",
    "        return self.observation, reward, terminated, None, None\n",
    "    \n",
    "    def get_current_demands(self, data, current_period):\n",
    "        \n",
    "        return data['demands'][:, :, current_period + self._internal_data['period_shift']]\n",
    "    \n",
    "    def calculate_store_reward_and_update_store_inventories(self, current_demands, action, observation, calculate_profit=False):\n",
    "        \"\"\"\n",
    "        Calculate reward and observation after demand and action is executed for stores\n",
    "        \"\"\"\n",
    "\n",
    "        store_inventory = self.observation['store_inventories']\n",
    "        inventory_on_hand = store_inventory[:, :, 0]\n",
    "        # print(f'current_demands.shape: {current_demands.shape}')\n",
    "        # print(f'store_inventories.shape: {self.observation[\"store_inventories\"][:, :, 0].shape}')\n",
    "        post_inventory_on_hand = self.observation['store_inventories'][:, :, 0] - current_demands\n",
    "\n",
    "        # reward given by underage_costs + holding_costs\n",
    "        if not calculate_profit:\n",
    "            reward = (\n",
    "                observation['underage_costs'] * torch.clip(-post_inventory_on_hand, min=0) + \n",
    "                observation['holding_costs'] * torch.clip(post_inventory_on_hand, min=0)\n",
    "                )\n",
    "        \n",
    "        # reward given by -sales*price - holding_costs\n",
    "        else:\n",
    "            reward = (\n",
    "                -observation['underage_costs'] * torch.minimum(inventory_on_hand, current_demands) + \n",
    "                observation['holding_costs'] * torch.clip(post_inventory_on_hand, min=0)\n",
    "                )\n",
    "        \n",
    "        if self.problem_params['lost_demand']:\n",
    "            post_inventory_on_hand = torch.clip(post_inventory_on_hand, min=0)\n",
    "\n",
    "        observation['store_inventories'] = self.update_inventory_for_heterogeneous_lead_times(\n",
    "            store_inventory, \n",
    "            post_inventory_on_hand, \n",
    "            action['stores'], \n",
    "            observation['lead_times'], \n",
    "            self._internal_data['allocation_shift']\n",
    "            )\n",
    "        \n",
    "        observation['warehouse_inventories'] = self.update_warehouse_inventory(\n",
    "            observation['warehouse_inventories'],\n",
    "            action['warehouses']\n",
    "            )\n",
    "        \n",
    "        print(f'observation[\"warehouse_lead_times\"]: {observation[\"warehouse_lead_times\"]}')\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def initialize_observation(self, data, observation_params):\n",
    "        \"\"\"\n",
    "        Initialize the observation of the environment\n",
    "        \"\"\"\n",
    "\n",
    "        observation = {\n",
    "            'store_inventories': data['initial_inventories'],\n",
    "            'current_period': torch.tensor([0])\n",
    "            }\n",
    "        \n",
    "        if observation_params['include_warehouse_inventory']:\n",
    "            observation['warehouse_inventories'] = data['initial_warehouse_inventories']\n",
    "\n",
    "        # include static features in observation (e.g., holding costs, underage costs, lead time and upper bounds)\n",
    "        for k, v in observation_params['include_static_features'].items():\n",
    "            if v:\n",
    "                observation[k] = data[k]\n",
    "\n",
    "        # initialize data for past observations of certain data (e.g., arrivals, orders)\n",
    "        for k, v in observation_params['include_past_observations'].items():\n",
    "            if v > 0:\n",
    "                observation[k] = torch.zeros(self.batch_size, self.n_stores, v).to(self.device)\n",
    "\n",
    "        if observation_params['demand']['past_periods'] > 0:\n",
    "            observation['past_demands'] = self.update_past_demands(data, observation_params, self.batch_size, self.n_stores, current_period=0)\n",
    "\n",
    "        if observation_params['include_days_to_christmas']:\n",
    "            observation['days_to_christmas'] = self.update_days_to_christmas(data, observation_params, current_period=0)\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def initialize_action_space(self, batch_size, problem_params, observation_params):\n",
    "        \"\"\"\n",
    "        Initialize the action space by creating a dict with spaces.Box with shape batch_size x locations\n",
    "        \"\"\"\n",
    "\n",
    "        d = {'stores': spaces.Box(low=0.0, high=np.inf, shape=(batch_size, problem_params['n_stores']), dtype=np.float32)}\n",
    "\n",
    "        for k1, k2 in zip(['warehouses', 'extra_echelons'], ['n_warehouses', 'n_extra_echelons']):\n",
    "            if problem_params[k2] > 0:\n",
    "                d[k1] = spaces.Box(low=0.0, high=np.inf, shape=(batch_size, problem_params[k2]), dtype=np.float32)\n",
    "\n",
    "        return spaces.Dict(d)\n",
    "    \n",
    "    def initialize_observation_space(self, initial_observation, periods, problem_params):\n",
    "        box_values = {\n",
    "            'arrivals': {'low': 0 if problem_params['lost_demand'] else -np.inf, 'high': np.inf, 'dtype': np.float32},\n",
    "            'holding_costs': {'low': 0, 'high': np.inf, 'dtype': np.float32},\n",
    "            'lead_times': {'low': 0, 'high': 2*10, 'dtype': np.int8},\n",
    "            'days_to_christmas': {'low': -365, 'high': 365, 'dtype': np.int8},\n",
    "            'orders': {'low': 0, 'high': np.inf, 'dtype': np.float32},\n",
    "            'past_demands': {'low': -np.inf, 'high': np.inf, 'dtype': np.float32},\n",
    "            'store_inventories': {'low': 0 if problem_params['lost_demand'] else -np.inf, 'high': np.inf, 'dtype': np.float32},\n",
    "            'warehouse_inventories': {'low': 0, 'high': np.inf, 'dtype': np.float32},\n",
    "            'extra_echelons_inventories': {'low': 0, 'high': np.inf, 'dtype': np.float32},\n",
    "            'underage_costs': {'low': 0, 'high': np.inf, 'dtype': np.float32},\n",
    "            'past_demands': {'low': -np.inf, 'high': np.inf, 'dtype': np.float32},\n",
    "            'current_period': {'low': 0, 'high': periods, 'dtype': np.int8},\n",
    "        }\n",
    "\n",
    "        return spaces.Dict(\n",
    "            {\n",
    "            k: spaces.Box(\n",
    "                low=box_values[k]['low'], \n",
    "                high=box_values[k]['high'], \n",
    "                shape=v.shape,\n",
    "                dtype=box_values[k]['dtype']\n",
    "                ) \n",
    "                for k, v in initial_observation.items()\n",
    "                })\n",
    "    \n",
    "    def update_inventory_for_heterogeneous_lead_times(self, inventory, inventory_on_hand, allocation, lead_times, allocation_shifter):\n",
    "        \"\"\"\n",
    "        Update the inventory for heterogeneous lead times (something simpler can be done for homogeneous lead times).\n",
    "        We add the inventory into corresponding position by flatenning out the state vector of the\n",
    "        entire batch. We create allocation_shifts earlier, which dictates the position shift of that long vector\n",
    "        for each store and each sample. We then add the corresponding lead time to obtain the actual position in \n",
    "        which to insert the action\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.stack(\n",
    "            [\n",
    "                inventory_on_hand + inventory[:, :, 1], \n",
    "                *self.move_columns_left(inventory, 1, inventory.shape[2] - 1), \n",
    "                torch.zeros_like(allocation)\n",
    "                ], \n",
    "                dim=2\n",
    "                ).put(\n",
    "                    (allocation_shifter + lead_times.long() - 1).flatten(), \n",
    "                    allocation.flatten(), \n",
    "                    accumulate=True\n",
    "                    )\n",
    "\n",
    "    def update_warehouse_inventory(self, warehouse_inventory, action):\n",
    "        \"\"\"\n",
    "        Update the warehouse inventory\n",
    "        \"\"\"\n",
    "\n",
    "        return warehouse_inventory + action\n",
    "\n",
    "    def update_past_demands(self, data, observation_params, batch_size, stores, current_period):\n",
    "        \"\"\"\n",
    "        Update the past demands in the observation\n",
    "        \"\"\"\n",
    "        \n",
    "        past_periods = observation_params['demand']['past_periods']\n",
    "        current_period_shifted = current_period + self._internal_data['period_shift']\n",
    "        \n",
    "        if current_period_shifted == 0:\n",
    "            past_demands = torch.zeros(batch_size, stores, past_periods).to(self.device)\n",
    "        else:\n",
    "            past_demands = data['demands'][:, :, max(0, current_period_shifted - past_periods): current_period_shifted]\n",
    "\n",
    "            fill_with_zeros = past_periods - (current_period_shifted - max(0, current_period_shifted - past_periods))\n",
    "            if fill_with_zeros > 0:\n",
    "                past_demands = torch.cat([\n",
    "                    torch.zeros(batch_size, stores, fill_with_zeros).to(self.device), \n",
    "                    past_demands\n",
    "                    ], \n",
    "                    dim=2)\n",
    "        \n",
    "        return past_demands\n",
    "    \n",
    "    def update_days_to_christmas(self, data, observation_params, current_period):\n",
    "        \"\"\"\n",
    "        Update the days to christmas in the observation\n",
    "        \"\"\"\n",
    "        days_to_christmas = data['days_to_christmas'][current_period + observation_params['demand']['period_shift']]\n",
    "        \n",
    "        return days_to_christmas\n",
    "\n",
    "    def move_columns_left(self, tensor_to_displace, start_index, end_index):\n",
    "        \"\"\"\n",
    "        move all columns in given array to the left, and return as list\n",
    "        \"\"\"\n",
    "\n",
    "        return [tensor_to_displace[:, :, i + 1] for i in range(start_index, end_index)]\n",
    "    \n",
    "    def update_observation(self, observation, action, demand):\n",
    "        \"\"\"\n",
    "        Update the observation of the environment\n",
    "        \"\"\"\n",
    "        inventory_on_hand = observation[:, :,  0] - demand\n",
    "        if self.lost_demand:\n",
    "            inventory_on_hand = torch.clip(inventory_on_hand, min=0)\n",
    "        \n",
    "        return torch.stack([inventory_on_hand + observation[:, :, 1],\n",
    "                            *self.move_columns_left(observation, 1, self.lead_time - 1),\n",
    "                            action],\n",
    "                            dim=2\n",
    "                            )\n",
    "    \n",
    "    # def simulate_batch(self, model, demand_batch):\n",
    "    #     \"\"\"\n",
    "    #     Simulate a batch of demand data\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     # initialize observation as a matrix of zeros\n",
    "    #     observation = torch.zeros(demand_batch.shape[0], demand_batch.shape[1], self.lead_time).to(self.device)\n",
    "    #     # print(f'initial observation: {observation.shape}')\n",
    "    #     # initialize reward across batch\n",
    "    #     batch_reward = 0\n",
    "    #     reward_to_report = 0\n",
    "\n",
    "    #     # loop through periods\n",
    "    #     for period in range(self.periods):\n",
    "    #         # get demand\n",
    "    #         demand = demand_batch[:, :, period]\n",
    "    #         # get action (i.e, order quantity)\n",
    "    #         action = model({'x': observation})\n",
    "    #         # action = model(observation)\n",
    "    #         # print(f'observation: {observation[0]}')\n",
    "    #         # print(f'action: {action[0]}')\n",
    "    #         # print(f'demand: {demand[0]}')\n",
    "    #         # print()\n",
    "    #         # get new observation and reward\n",
    "    #         observation, reward = self.step(observation, action, demand)\n",
    "            \n",
    "\n",
    "    #         batch_reward += reward\n",
    "    #         # add reward to batch reward only after lead time has passed (as costs on first periods do not depend on agent's actions)\n",
    "    #         if period >= 20:\n",
    "    #         # if period >= self.lead_time:\n",
    "    #             reward_to_report += reward\n",
    "\n",
    "    #     # return reward\n",
    "    #     return batch_reward, reward_to_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'demand'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m warehouse_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mholding_cost\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m, \n\u001b[1;32m     49\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlead_time\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m}\n\u001b[1;32m     51\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m scenario \u001b[38;5;241m=\u001b[39m \u001b[43mScenario\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarehouse_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m creator \u001b[38;5;241m=\u001b[39m DatasetCreator()\n\u001b[1;32m     54\u001b[0m train_dataset, dev_dataset \u001b[38;5;241m=\u001b[39m creator\u001b[38;5;241m.\u001b[39mcreate_datasets(scenario, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, by_sample_indexes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_index_for_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m)\n",
      "File \u001b[0;32m<string>:15\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, periods, problem_params, store_params, warehouse_params, num_samples, seeds)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'demand'"
     ]
    }
   ],
   "source": [
    "# only execute if name is main\n",
    "if __name__ == '__main__':\n",
    "    # test the class\n",
    "    seeds = {\"underage_cost\": 28, \"holding_cost\": 73, \"mean\": 33, \"coef_of_var\": 92, \"lead_time\": 41, 'demand': 57, \"initial_inventory\": 88}\n",
    "    # \"seeds\": {\"underage\": 28, \"holding\": 73, \"mean\": 33, \"stds\": 92, \"demand_sequence\": 57, \"w_lead_time\": 88, \"perturbation\": 84, \"store_lead_times\": 41}\n",
    "    problem_params = {'n_stores': 10, 'n_warehouses': 1, 'n_extra_echelons': 0, 'periods': 50, 'lost_demand': False}\n",
    "    num_samples = 512\n",
    "    batch_size = 128\n",
    "    store_params = {'demand': {'sample_across_stores': True,\n",
    "                               'mean_range': [2.5, 7.5], \n",
    "                               'coef_of_var_range': [0.16, 0.32],\n",
    "                               'distribution': 'normal',\n",
    "                               'correlation': 0.5,\n",
    "                               'clip': True\n",
    "                               },\n",
    "                    \n",
    "                  #   'demand_1': {'sample_across_stores': False,\n",
    "                  #              'mean': [5.0], \n",
    "                  #              'distribution': 'poisson',\n",
    "                  #              'clip': True\n",
    "                  #              },\n",
    "\n",
    "                    'lead_time': {'sample_across_stores': True,\n",
    "                                   'vary_across_samples': False, \n",
    "                                   'range': [4, 7]\n",
    "                                #    'value': 4\n",
    "                                   },\n",
    "\n",
    "                    'holding_cost': {'sample_across_stores': False, \n",
    "                                      'vary_across_samples': False,\n",
    "                                      'expand': True,\n",
    "                                      'value': 1\n",
    "                                      },\n",
    "\n",
    "                    'underage_cost': {'sample_across_stores': True,\n",
    "                                       'vary_across_samples': False, \n",
    "                                       'expand': False, \n",
    "                                       # 'value': 5.0,\n",
    "                                       'range': [2.5, 7.5],\n",
    "                                       },\n",
    "\n",
    "                     'initial_inventory': {'sample': False,\n",
    "                                           'range_mult': [0, 1],\n",
    "                                           'inventory_periods': 6\n",
    "                                           }\n",
    "                    }\n",
    "    \n",
    "    warehouse_params = {'holding_cost': 0.3, \n",
    "                        'lead_time': 4}\n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    scenario = Scenario(problem_params, store_params, warehouse_params, num_samples, seeds)\n",
    "    creator = DatasetCreator()\n",
    "    train_dataset, dev_dataset = creator.create_datasets(scenario, split=True, by_sample_indexes=True, sample_index_for_split=400)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "    neurons_per_hidden_layer = [32, 32, 32]\n",
    "    model = FullyConnectedNN(neurons_per_hidden_layer, output_size=problem_params['n_stores']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    observation_params = {\n",
    "        'include_warehouse_inventory': False,\n",
    "        'include_static_features': {\n",
    "            'holding_costs': True, \n",
    "            'underage_costs': True, \n",
    "            'lead_times': True, \n",
    "            'upper_bounds': False\n",
    "            },\n",
    "        'demand': {\n",
    "            'past_periods': 0, \n",
    "            'period_shift': 0\n",
    "            },\n",
    "        'include_past_observations': {\n",
    "            'arrivals': 0, \n",
    "            'orders': 0\n",
    "            },\n",
    "        'include_days_to_christmas': False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    simulator = Simulator(device=device)\n",
    "    # simulator.reset(problem_params, train_dataset, observation_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0\n",
      "store_inventories: tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 1\n",
      "store_inventories: tensor([[-2.0828,  0.0000,  0.0000,  1.1274,  0.0000,  0.0000],\n",
      "        [-1.6807,  0.0000,  0.0000,  1.1841,  0.0000,  0.0000],\n",
      "        [-3.4447,  0.0000,  0.0000,  0.0000,  0.0000,  0.9343],\n",
      "        [-1.4319,  0.0000,  0.0000,  0.8862,  0.0000,  0.0000],\n",
      "        [-4.8511,  0.0000,  0.0000,  0.0000,  1.0472,  0.0000],\n",
      "        [-1.5835,  0.0000,  0.0000,  0.0000,  0.9497,  0.0000],\n",
      "        [-1.1774,  0.0000,  0.0000,  0.0000,  1.0687,  0.0000],\n",
      "        [-3.2697,  0.0000,  0.0000,  0.0000,  0.0000,  0.8615],\n",
      "        [-2.9201,  0.0000,  0.0000,  0.0000,  1.0025,  0.0000],\n",
      "        [-2.4170,  0.0000,  0.0000,  0.9289,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 2\n",
      "store_inventories: tensor([[ -3.5044,   0.0000,   1.1274,   1.1934,   0.0000,   0.0000],\n",
      "        [ -5.1536,   0.0000,   1.1841,   1.3448,   0.0000,   0.0000],\n",
      "        [ -6.9603,   0.0000,   0.0000,   0.0000,   0.9343,   0.8870],\n",
      "        [ -4.1378,   0.0000,   0.8862,   1.0408,   0.0000,   0.0000],\n",
      "        [-10.8286,   0.0000,   0.0000,   1.0472,   0.8937,   0.0000],\n",
      "        [ -3.8940,   0.0000,   0.0000,   0.9497,   0.7804,   0.0000],\n",
      "        [ -3.0953,   0.0000,   0.0000,   1.0687,   1.1411,   0.0000],\n",
      "        [ -7.5276,   0.0000,   0.0000,   0.0000,   0.8615,   0.8611],\n",
      "        [ -6.3429,   0.0000,   0.0000,   1.0025,   0.9655,   0.0000],\n",
      "        [ -3.2844,   0.0000,   0.9289,   1.0855,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 3\n",
      "store_inventories: tensor([[ -6.6233,   1.1274,   1.1934,   1.1697,   0.0000,   0.0000],\n",
      "        [-10.5260,   1.1841,   1.3448,   1.5596,   0.0000,   0.0000],\n",
      "        [-11.7040,   0.0000,   0.0000,   0.9343,   0.8870,   0.8580],\n",
      "        [ -7.6274,   0.8862,   1.0408,   1.2478,   0.0000,   0.0000],\n",
      "        [-17.0777,   0.0000,   1.0472,   0.8937,   0.6576,   0.0000],\n",
      "        [ -6.0782,   0.0000,   0.9497,   0.7804,   0.6273,   0.0000],\n",
      "        [ -5.3989,   0.0000,   1.0687,   1.1411,   1.1723,   0.0000],\n",
      "        [-15.4748,   0.0000,   0.0000,   0.8615,   0.8611,   0.8420],\n",
      "        [-11.9864,   0.0000,   1.0025,   0.9655,   0.9211,   0.0000],\n",
      "        [ -7.8234,   0.9289,   1.0855,   1.3121,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 4\n",
      "store_inventories: tensor([[ -8.5561,   1.1934,   1.1697,   1.0691,   0.0000,   0.0000],\n",
      "        [-12.3749,   1.3448,   1.5596,   1.7441,   0.0000,   0.0000],\n",
      "        [-15.8312,   0.0000,   0.9343,   0.8870,   0.8580,   0.8625],\n",
      "        [ -9.6263,   1.0408,   1.2478,   1.4803,   0.0000,   0.0000],\n",
      "        [-24.9678,   1.0472,   0.8937,   0.6576,   0.3832,   0.0000],\n",
      "        [ -8.5196,   0.9497,   0.7804,   0.6273,   0.4574,   0.0000],\n",
      "        [ -7.5839,   1.0687,   1.1411,   1.1723,   1.1849,   0.0000],\n",
      "        [-21.1629,   0.0000,   0.8615,   0.8611,   0.8420,   0.8556],\n",
      "        [-19.6159,   1.0025,   0.9655,   0.9211,   0.8488,   0.0000],\n",
      "        [-12.4045,   1.0855,   1.3121,   1.5322,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 5\n",
      "store_inventories: tensor([[-13.0197,   1.1697,   1.0691,   1.0166,   0.0000,   0.0000],\n",
      "        [-18.0216,   1.5596,   1.7441,   1.8542,   0.0000,   0.0000],\n",
      "        [-21.4545,   0.9343,   0.8870,   0.8580,   0.8625,   0.8235],\n",
      "        [-14.0724,   1.2478,   1.4803,   1.6631,   0.0000,   0.0000],\n",
      "        [-31.6272,   0.8937,   0.6576,   0.3832,   0.1976,   0.0000],\n",
      "        [-11.5556,   0.7804,   0.6273,   0.4574,   0.3511,   0.0000],\n",
      "        [ -9.5608,   1.1411,   1.1723,   1.1849,   1.1976,   0.0000],\n",
      "        [-29.5790,   0.8615,   0.8611,   0.8420,   0.8556,   0.8672],\n",
      "        [-26.1147,   0.9655,   0.9211,   0.8488,   0.7602,   0.0000],\n",
      "        [-18.2463,   1.3121,   1.5322,   1.6905,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 6\n",
      "store_inventories: tensor([[-15.4132,   1.0691,   1.0166,   0.8434,   0.0000,   0.0000],\n",
      "        [-22.0720,   1.7441,   1.8542,   1.9646,   0.0000,   0.0000],\n",
      "        [-25.5226,   0.8870,   0.8580,   0.8625,   0.8235,   0.8221],\n",
      "        [-17.6688,   1.4803,   1.6631,   1.8683,   0.0000,   0.0000],\n",
      "        [-37.6984,   0.6576,   0.3832,   0.1976,   0.0000,   0.0000],\n",
      "        [-14.3380,   0.6273,   0.4574,   0.3511,   0.2533,   0.0000],\n",
      "        [-11.6640,   1.1723,   1.1849,   1.1976,   1.1240,   0.0000],\n",
      "        [-36.9733,   0.8611,   0.8420,   0.8556,   0.8672,   0.9032],\n",
      "        [-31.0063,   0.9211,   0.8488,   0.7602,   0.6936,   0.0000],\n",
      "        [-21.7638,   1.5322,   1.6905,   1.9092,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 7\n",
      "store_inventories: tensor([[-16.6545,   1.0166,   0.8434,   0.7197,   0.0000,   0.0000],\n",
      "        [-27.0222,   1.8542,   1.9646,   2.0143,   0.0000,   0.0000],\n",
      "        [-30.1340,   0.8580,   0.8625,   0.8235,   0.8221,   0.8222],\n",
      "        [-19.9321,   1.6631,   1.8683,   2.0214,   0.0000,   0.0000],\n",
      "        [-44.5018,   0.3832,   0.1976,   0.0000,   0.0000,   0.0000],\n",
      "        [-18.3247,   0.4574,   0.3511,   0.2533,   0.1694,   0.0000],\n",
      "        [-12.5304,   1.1849,   1.1976,   1.1240,   1.0443,   0.0000],\n",
      "        [-46.1687,   0.8420,   0.8556,   0.8672,   0.9032,   0.9349],\n",
      "        [-37.6220,   0.8488,   0.7602,   0.6936,   0.6578,   0.0000],\n",
      "        [-24.6306,   1.6905,   1.9092,   2.0649,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 8\n",
      "store_inventories: tensor([[-20.8284,   0.8434,   0.7197,   0.6320,   0.0000,   0.0000],\n",
      "        [-30.2016,   1.9646,   2.0143,   2.0487,   0.0000,   0.0000],\n",
      "        [-34.0091,   0.8625,   0.8235,   0.8221,   0.8222,   0.8232],\n",
      "        [-22.6985,   1.8683,   2.0214,   2.1993,   0.0000,   0.0000],\n",
      "        [-50.3841,   0.1976,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-23.9620,   0.3511,   0.2533,   0.1694,   0.0758,   0.0000],\n",
      "        [-14.6139,   1.1976,   1.1240,   1.0443,   0.9542,   0.0000],\n",
      "        [-57.0245,   0.8556,   0.8672,   0.9032,   0.9349,   0.9004],\n",
      "        [-42.9397,   0.7602,   0.6936,   0.6578,   0.5986,   0.0000],\n",
      "        [-29.3367,   1.9092,   2.0649,   2.1903,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 9\n",
      "store_inventories: tensor([[-20.5723,   0.7197,   0.6320,   0.5202,   0.0000,   0.0000],\n",
      "        [-31.3952,   2.0143,   2.0487,   2.1034,   0.0000,   0.0000],\n",
      "        [-35.7770,   0.8235,   0.8221,   0.8222,   0.8232,   0.7780],\n",
      "        [-22.4118,   2.0214,   2.1993,   2.3856,   0.0000,   0.0000],\n",
      "        [-54.5824,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-24.8982,   0.2533,   0.1694,   0.0758,   0.0000,   0.0000],\n",
      "        [-15.6421,   1.1240,   1.0443,   0.9542,   0.7929,   0.0000],\n",
      "        [-59.3469,   0.8672,   0.9032,   0.9349,   0.9004,   0.9280],\n",
      "        [-44.7012,   0.6936,   0.6578,   0.5986,   0.5362,   0.0000],\n",
      "        [-29.2268,   2.0649,   2.1903,   2.3641,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 10\n",
      "store_inventories: tensor([[-25.2719,   0.6320,   0.5202,   0.4833,   0.0000,   0.0000],\n",
      "        [-34.5427,   2.0487,   2.1034,   2.0990,   0.0000,   0.0000],\n",
      "        [-39.5363,   0.8221,   0.8222,   0.8232,   0.7780,   0.7862],\n",
      "        [-24.8732,   2.1993,   2.3856,   2.4420,   0.0000,   0.0000],\n",
      "        [-61.2395,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-28.8855,   0.1694,   0.0758,   0.0000,   0.0000,   0.0000],\n",
      "        [-17.2997,   1.0443,   0.9542,   0.7929,   0.7829,   0.0000],\n",
      "        [-66.1176,   0.9032,   0.9349,   0.9004,   0.9280,   0.9139],\n",
      "        [-48.3918,   0.6578,   0.5986,   0.5362,   0.5082,   0.0000],\n",
      "        [-32.2398,   2.1903,   2.3641,   2.4092,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 11\n",
      "store_inventories: tensor([[-28.9683,   0.5202,   0.4833,   0.3717,   0.0000,   0.0000],\n",
      "        [-37.4064,   2.1034,   2.0990,   2.1695,   0.0000,   0.0000],\n",
      "        [-43.8383,   0.8222,   0.8232,   0.7780,   0.7862,   0.7294],\n",
      "        [-26.8864,   2.3856,   2.4420,   2.6126,   0.0000,   0.0000],\n",
      "        [-66.5350,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-31.7941,   0.0758,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-19.1312,   0.9542,   0.7929,   0.7829,   0.6576,   0.0000],\n",
      "        [-71.7459,   0.9349,   0.9004,   0.9280,   0.9139,   0.9632],\n",
      "        [-53.6317,   0.5986,   0.5362,   0.5082,   0.4608,   0.0000],\n",
      "        [-34.2561,   2.3641,   2.4092,   2.5899,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 12\n",
      "store_inventories: tensor([[-32.8916,   0.4833,   0.3717,   0.2921,   0.0000,   0.0000],\n",
      "        [-40.0881,   2.0990,   2.1695,   2.2267,   0.0000,   0.0000],\n",
      "        [-48.1963,   0.8232,   0.7780,   0.7862,   0.7294,   0.7091],\n",
      "        [-29.7461,   2.4420,   2.6126,   2.7458,   0.0000,   0.0000],\n",
      "        [-73.2986,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-36.3441,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-19.9917,   0.7929,   0.7829,   0.6576,   0.5880,   0.0000],\n",
      "        [-79.6066,   0.9004,   0.9280,   0.9139,   0.9632,   0.9783],\n",
      "        [-58.4767,   0.5362,   0.5082,   0.4608,   0.4081,   0.0000],\n",
      "        [-36.6961,   2.4092,   2.5899,   2.7050,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 13\n",
      "store_inventories: tensor([[-34.5818,   0.3717,   0.2921,   0.2038,   0.0000,   0.0000],\n",
      "        [-41.4696,   2.1695,   2.2267,   2.2878,   0.0000,   0.0000],\n",
      "        [-51.9528,   0.7780,   0.7862,   0.7294,   0.7091,   0.6627],\n",
      "        [-30.0359,   2.6126,   2.7458,   2.9226,   0.0000,   0.0000],\n",
      "        [-78.5105,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-39.2765,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-21.2507,   0.7829,   0.6576,   0.5880,   0.4725,   0.0000],\n",
      "        [-84.6685,   0.9280,   0.9139,   0.9632,   0.9783,   0.9959],\n",
      "        [-63.0230,   0.5082,   0.4608,   0.4081,   0.3538,   0.0000],\n",
      "        [-38.6020,   2.5899,   2.7050,   2.8516,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 14\n",
      "store_inventories: tensor([[-37.6579,   0.2921,   0.2038,   0.1513,   0.0000,   0.0000],\n",
      "        [-42.4551,   2.2267,   2.2878,   2.3193,   0.0000,   0.0000],\n",
      "        [-55.4159,   0.7862,   0.7294,   0.7091,   0.6627,   0.6509],\n",
      "        [-31.6821,   2.7458,   2.9226,   3.0343,   0.0000,   0.0000],\n",
      "        [-85.6718,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-41.6225,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-23.2243,   0.6576,   0.5880,   0.4725,   0.4238,   0.0000],\n",
      "        [-89.9714,   0.9139,   0.9632,   0.9783,   0.9959,   0.9874],\n",
      "        [-68.0253,   0.4608,   0.4081,   0.3538,   0.3047,   0.0000],\n",
      "        [-40.8854,   2.7050,   2.8516,   2.9247,   0.0000,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 15\n",
      "store_inventories: tensor([[-4.1343e+01,  2.0383e-01,  1.5126e-01,  7.3241e-02,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-4.5533e+01,  2.2878e+00,  2.3193e+00,  2.3566e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.9731e+01,  7.2937e-01,  7.0914e-01,  6.6266e-01,  6.5092e-01,\n",
      "          6.1710e-01],\n",
      "        [-3.2730e+01,  2.9226e+00,  3.0343e+00,  3.1709e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-9.1975e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-4.4369e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.6087e+01,  5.8796e-01,  4.7252e-01,  4.2383e-01,  3.4910e-01,\n",
      "          0.0000e+00],\n",
      "        [-9.9272e+01,  9.6319e-01,  9.7829e-01,  9.9594e-01,  9.8737e-01,\n",
      "          1.0075e+00],\n",
      "        [-7.4640e+01,  4.0809e-01,  3.5378e-01,  3.0469e-01,  2.5009e-01,\n",
      "          0.0000e+00],\n",
      "        [-4.2848e+01,  2.8516e+00,  2.9247e+00,  3.0366e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 16\n",
      "store_inventories: tensor([[-4.4759e+01,  1.5126e-01,  7.3241e-02,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-4.8331e+01,  2.3193e+00,  2.3566e+00,  2.3820e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.3873e+01,  7.0914e-01,  6.6266e-01,  6.5092e-01,  6.1710e-01,\n",
      "          5.9030e-01],\n",
      "        [-3.3820e+01,  3.0343e+00,  3.1709e+00,  3.3226e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-9.8923e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-4.7853e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.8549e+01,  4.7252e-01,  4.2383e-01,  3.4910e-01,  2.3335e-01,\n",
      "          0.0000e+00],\n",
      "        [-1.0481e+02,  9.7829e-01,  9.9594e-01,  9.8737e-01,  1.0075e+00,\n",
      "          1.0085e+00],\n",
      "        [-8.2221e+01,  3.5378e-01,  3.0469e-01,  2.5009e-01,  1.6658e-01,\n",
      "          0.0000e+00],\n",
      "        [-4.4203e+01,  2.9247e+00,  3.0366e+00,  3.1388e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 17\n",
      "store_inventories: tensor([[-4.7061e+01,  7.3241e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.0011e+01,  2.3566e+00,  2.3820e+00,  2.4198e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.6253e+01,  6.6266e-01,  6.5092e-01,  6.1710e-01,  5.9030e-01,\n",
      "          5.5731e-01],\n",
      "        [-3.3842e+01,  3.1709e+00,  3.3226e+00,  3.4622e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.0445e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.0124e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.9838e+01,  4.2383e-01,  3.4910e-01,  2.3335e-01,  1.5164e-01,\n",
      "          0.0000e+00],\n",
      "        [-1.0898e+02,  9.9594e-01,  9.8737e-01,  1.0075e+00,  1.0085e+00,\n",
      "          1.0065e+00],\n",
      "        [-8.6185e+01,  3.0469e-01,  2.5009e-01,  1.6658e-01,  1.0369e-01,\n",
      "          0.0000e+00],\n",
      "        [-4.5635e+01,  3.0366e+00,  3.1388e+00,  3.2251e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 18\n",
      "store_inventories: tensor([[-4.9585e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.1596e+01,  2.3820e+00,  2.4198e+00,  2.4429e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.9364e+01,  6.5092e-01,  6.1710e-01,  5.9030e-01,  5.5731e-01,\n",
      "          5.2211e-01],\n",
      "        [-3.4062e+01,  3.3226e+00,  3.4622e+00,  3.5672e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.1023e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.3801e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-3.2184e+01,  3.4910e-01,  2.3335e-01,  1.5164e-01,  7.5266e-02,\n",
      "          0.0000e+00],\n",
      "        [-1.1473e+02,  9.8737e-01,  1.0075e+00,  1.0085e+00,  1.0065e+00,\n",
      "          1.0164e+00],\n",
      "        [-9.1940e+01,  2.5009e-01,  1.6658e-01,  1.0369e-01,  5.9516e-02,\n",
      "          0.0000e+00],\n",
      "        [-4.7967e+01,  3.1388e+00,  3.2251e+00,  3.2999e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 19\n",
      "store_inventories: tensor([[-5.2691e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.1105e+01,  2.4198e+00,  2.4429e+00,  2.4711e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-7.2414e+01,  6.1710e-01,  5.9030e-01,  5.5731e-01,  5.2211e-01,\n",
      "          4.8639e-01],\n",
      "        [-3.3503e+01,  3.4622e+00,  3.5672e+00,  3.6851e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.1666e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6658e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-3.4422e+01,  2.3335e-01,  1.5164e-01,  7.5266e-02,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.2013e+02,  1.0075e+00,  1.0085e+00,  1.0065e+00,  1.0164e+00,\n",
      "          1.0236e+00],\n",
      "        [-9.5333e+01,  1.6658e-01,  1.0369e-01,  5.9516e-02,  3.9220e-03,\n",
      "          0.0000e+00],\n",
      "        [-4.8502e+01,  3.2251e+00,  3.2999e+00,  3.3818e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 20\n",
      "store_inventories: tensor([[-5.6608e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.2885e+01,  2.4429e+00,  2.4711e+00,  2.4921e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-7.5685e+01,  5.9030e-01,  5.5731e-01,  5.2211e-01,  4.8639e-01,\n",
      "          4.3180e-01],\n",
      "        [-3.4932e+01,  3.5672e+00,  3.6851e+00,  3.7914e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.2306e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.1460e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-3.6715e+01,  1.5164e-01,  7.5266e-02,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.2753e+02,  1.0085e+00,  1.0065e+00,  1.0164e+00,  1.0236e+00,\n",
      "          1.0448e+00],\n",
      "        [-1.0111e+02,  1.0369e-01,  5.9516e-02,  3.9220e-03,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.1368e+01,  3.2999e+00,  3.3818e+00,  3.4640e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 21\n",
      "store_inventories: tensor([[-5.8825e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.4577e+01,  2.4711e+00,  2.4921e+00,  2.5339e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-7.9479e+01,  5.5731e-01,  5.2211e-01,  4.8639e-01,  4.3180e-01,\n",
      "          3.8148e-01],\n",
      "        [-3.4369e+01,  3.6851e+00,  3.7914e+00,  3.9364e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.3007e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.4884e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-3.9872e+01,  7.5266e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.3662e+02,  1.0065e+00,  1.0164e+00,  1.0236e+00,  1.0448e+00,\n",
      "          1.0615e+00],\n",
      "        [-1.0746e+02,  5.9516e-02,  3.9220e-03,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.2886e+01,  3.3818e+00,  3.4640e+00,  3.5792e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 22\n",
      "store_inventories: tensor([[-6.3369e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6730e+01,  2.4921e+00,  2.5339e+00,  2.5419e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-8.2025e+01,  5.2211e-01,  4.8639e-01,  4.3180e-01,  3.8148e-01,\n",
      "          3.3956e-01],\n",
      "        [-3.4164e+01,  3.7914e+00,  3.9364e+00,  4.0739e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.3576e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.9316e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-4.1665e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.4390e+02,  1.0164e+00,  1.0236e+00,  1.0448e+00,  1.0615e+00,\n",
      "          1.0657e+00],\n",
      "        [-1.1363e+02,  3.9220e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.4511e+01,  3.4640e+00,  3.5792e+00,  3.6560e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 23\n",
      "store_inventories: tensor([[ -68.3298,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -60.1951,    2.5339,    2.5419,    2.5780,    0.0000,    0.0000],\n",
      "        [ -87.7933,    0.4864,    0.4318,    0.3815,    0.3396,    0.2880],\n",
      "        [ -35.7750,    3.9364,    4.0739,    4.2061,    0.0000,    0.0000],\n",
      "        [-143.0258,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -72.7456,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -45.4175,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-151.9238,    1.0236,    1.0448,    1.0615,    1.0657,    1.0636],\n",
      "        [-120.2120,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -56.3758,    3.5792,    3.6560,    3.7383,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 24\n",
      "store_inventories: tensor([[ -71.9099,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -62.8458,    2.5419,    2.5780,    2.6302,    0.0000,    0.0000],\n",
      "        [ -90.7837,    0.4318,    0.3815,    0.3396,    0.2880,    0.2505],\n",
      "        [ -35.4665,    4.0739,    4.2061,    4.3586,    0.0000,    0.0000],\n",
      "        [-149.9432,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -76.8438,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -48.0892,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-157.1314,    1.0448,    1.0615,    1.0657,    1.0636,    1.1004],\n",
      "        [-126.0045,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -55.9001,    3.6560,    3.7383,    3.8820,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 25\n",
      "store_inventories: tensor([[ -74.6365,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -63.8540,    2.5780,    2.6302,    2.6565,    0.0000,    0.0000],\n",
      "        [ -94.8963,    0.3815,    0.3396,    0.2880,    0.2505,    0.2008],\n",
      "        [ -34.9610,    4.2061,    4.3586,    4.4757,    0.0000,    0.0000],\n",
      "        [-155.6826,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -79.9007,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -49.9274,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-163.3369,    1.0615,    1.0657,    1.0636,    1.1004,    1.1048],\n",
      "        [-130.1060,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -55.7902,    3.7383,    3.8820,    3.9562,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 26\n",
      "store_inventories: tensor([[-7.8895e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.5726e+01,  2.6302e+00,  2.6565e+00,  2.6837e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-9.7046e+01,  3.3956e-01,  2.8797e-01,  2.5048e-01,  2.0080e-01,\n",
      "          1.5504e-01],\n",
      "        [-3.4011e+01,  4.3586e+00,  4.4757e+00,  4.5811e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.6301e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-8.3659e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.2188e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.6795e+02,  1.0657e+00,  1.0636e+00,  1.1004e+00,  1.1048e+00,\n",
      "          1.1195e+00],\n",
      "        [-1.3528e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6755e+01,  3.8820e+00,  3.9562e+00,  4.0286e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 27\n",
      "store_inventories: tensor([[-8.3382e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.7590e+01,  2.6565e+00,  2.6837e+00,  2.7181e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.0016e+02,  2.8797e-01,  2.5048e-01,  2.0080e-01,  1.5504e-01,\n",
      "          9.5505e-02],\n",
      "        [-3.3119e+01,  4.4757e+00,  4.5811e+00,  4.6989e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.6975e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-8.6671e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.4005e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.7501e+02,  1.0636e+00,  1.1004e+00,  1.1048e+00,  1.1195e+00,\n",
      "          1.1376e+00],\n",
      "        [-1.3939e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6172e+01,  3.9562e+00,  4.0286e+00,  4.1225e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 28\n",
      "store_inventories: tensor([[-8.7470e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.6278e+01,  2.6837e+00,  2.7181e+00,  2.7455e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.0478e+02,  2.5048e-01,  2.0080e-01,  1.5504e-01,  9.5505e-02,\n",
      "          3.2938e-02],\n",
      "        [-3.2318e+01,  4.5811e+00,  4.6989e+00,  4.8202e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.7489e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-9.0124e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6316e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.8247e+02,  1.1004e+00,  1.1048e+00,  1.1195e+00,  1.1376e+00,\n",
      "          1.1579e+00],\n",
      "        [-1.4484e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.5206e+01,  4.0286e+00,  4.1225e+00,  4.2122e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 29\n",
      "store_inventories: tensor([[-9.1171e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.8801e+01,  2.7181e+00,  2.7455e+00,  2.7729e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.0793e+02,  2.0080e-01,  1.5504e-01,  9.5505e-02,  3.2938e-02,\n",
      "          0.0000e+00],\n",
      "        [-2.9822e+01,  4.6989e+00,  4.8202e+00,  4.9057e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.8087e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-9.3042e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.8348e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.8961e+02,  1.1048e+00,  1.1195e+00,  1.1376e+00,  1.1579e+00,\n",
      "          1.1711e+00],\n",
      "        [-1.5085e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.5622e+01,  4.1225e+00,  4.2122e+00,  4.2622e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 30\n",
      "store_inventories: tensor([[-9.6164e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-7.1017e+01,  2.7455e+00,  2.7729e+00,  2.7989e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.1239e+02,  1.5504e-01,  9.5505e-02,  3.2938e-02,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.8619e+01,  4.8202e+00,  4.9057e+00,  5.0168e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.8872e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-9.6695e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.1134e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.9656e+02,  1.1195e+00,  1.1376e+00,  1.1579e+00,  1.1711e+00,\n",
      "          1.1756e+00],\n",
      "        [-1.5731e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6473e+01,  4.2122e+00,  4.2622e+00,  4.3276e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 31\n",
      "store_inventories: tensor([[-9.9825e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-7.4987e+01,  2.7729e+00,  2.7989e+00,  2.8459e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.1738e+02,  9.5505e-02,  3.2938e-02,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.8161e+01,  4.9057e+00,  5.0168e+00,  5.1487e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.9651e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-9.9239e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.3369e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.0490e+02,  1.1376e+00,  1.1579e+00,  1.1711e+00,  1.1756e+00,\n",
      "          1.2027e+00],\n",
      "        [-1.6251e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6537e+01,  4.2622e+00,  4.3276e+00,  4.4401e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 32\n",
      "store_inventories: tensor([[-1.0138e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-7.4931e+01,  2.7989e+00,  2.8459e+00,  2.8821e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.2243e+02,  3.2938e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.6151e+01,  5.0168e+00,  5.1487e+00,  5.3020e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.0341e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-1.0244e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-6.5948e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.0961e+02,  1.1579e+00,  1.1711e+00,  1.1756e+00,  1.2027e+00,\n",
      "          1.2300e+00],\n",
      "        [-1.6731e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-5.6537e+01,  4.3276e+00,  4.4401e+00,  4.5527e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 33\n",
      "store_inventories: tensor([[-104.1906,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -74.0385,    2.8459,    2.8821,    2.9111,    0.0000,    0.0000],\n",
      "        [-126.5460,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -23.4161,    5.1487,    5.3020,    5.3836,    0.0000,    0.0000],\n",
      "        [-208.8617,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-105.1920,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -67.9334,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-213.7286,    1.1711,    1.1756,    1.2027,    1.2300,    1.2588],\n",
      "        [-170.3836,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -56.1933,    4.4401,    4.5527,    4.6179,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 34\n",
      "store_inventories: tensor([[-109.1748,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -76.0390,    2.8821,    2.9111,    2.9454,    0.0000,    0.0000],\n",
      "        [-129.9784,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -21.3577,    5.3020,    5.3836,    5.4421,    0.0000,    0.0000],\n",
      "        [-216.6229,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-109.2378,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -70.8509,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-220.4612,    1.1756,    1.2027,    1.2300,    1.2588,    1.2942],\n",
      "        [-177.2115,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -57.2182,    4.5527,    4.6179,    4.6879,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 35\n",
      "store_inventories: tensor([[-111.9667,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -77.5599,    2.9111,    2.9454,    2.9868,    0.0000,    0.0000],\n",
      "        [-134.4773,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -19.7030,    5.3836,    5.4421,    5.5593,    0.0000,    0.0000],\n",
      "        [-222.3040,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-109.9479,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -73.2185,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-225.4039,    1.2027,    1.2300,    1.2588,    1.2942,    1.3154],\n",
      "        [-180.9582,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -57.8714,    4.6179,    4.6879,    4.7933,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 36\n",
      "store_inventories: tensor([[-116.2778,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -81.2001,    2.9454,    2.9868,    3.0147,    0.0000,    0.0000],\n",
      "        [-139.3013,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -18.7053,    5.4421,    5.5593,    5.6418,    0.0000,    0.0000],\n",
      "        [-229.5784,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-113.9096,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -75.9926,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-231.9928,    1.2300,    1.2588,    1.2942,    1.3154,    1.3528],\n",
      "        [-188.1605,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -59.5309,    4.6879,    4.7933,    4.8749,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 37\n",
      "store_inventories: tensor([[-121.8088,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -83.6027,    2.9868,    3.0147,    3.0663,    0.0000,    0.0000],\n",
      "        [-146.2574,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -19.5892,    5.5593,    5.6418,    5.7764,    0.0000,    0.0000],\n",
      "        [-237.8575,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-119.4399,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -79.3894,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-240.1029,    1.2588,    1.2942,    1.3154,    1.3528,    1.3722],\n",
      "        [-192.8960,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -60.5910,    4.7933,    4.8749,    4.9843,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 38\n",
      "store_inventories: tensor([[-124.2072,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -85.3450,    3.0147,    3.0663,    3.1430,    0.0000,    0.0000],\n",
      "        [-151.3311,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -16.0338,    5.6418,    5.7764,    5.9332,    0.0000,    0.0000],\n",
      "        [-243.2272,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-122.0469,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -81.6682,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-244.3580,    1.2942,    1.3154,    1.3528,    1.3722,    1.4293],\n",
      "        [-199.0865,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -59.7842,    4.8749,    4.9843,    5.1616,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 39\n",
      "store_inventories: tensor([[-127.9043,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -85.3059,    3.0663,    3.1430,    3.1744,    0.0000,    0.0000],\n",
      "        [-156.0315,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -14.5685,    5.7764,    5.9332,    5.9936,    0.0000,    0.0000],\n",
      "        [-250.2570,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-125.9716,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -84.4619,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-252.1272,    1.3154,    1.3528,    1.3722,    1.4293,    1.4358],\n",
      "        [-205.3439,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -61.0718,    4.9843,    5.1616,    5.1986,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 40\n",
      "store_inventories: tensor([[-131.4323,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -87.9391,    3.1430,    3.1744,    3.2059,    0.0000,    0.0000],\n",
      "        [-159.7281,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -12.6692,    5.9332,    5.9936,    6.0985,    0.0000,    0.0000],\n",
      "        [-257.8408,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-129.4061,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -87.0365,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-257.7404,    1.3528,    1.3722,    1.4293,    1.4358,    1.4580],\n",
      "        [-213.0041,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -62.3320,    5.1616,    5.1986,    5.2900,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 41\n",
      "store_inventories: tensor([[-135.9390,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -89.1038,    3.1744,    3.2059,    3.2434,    0.0000,    0.0000],\n",
      "        [-164.3969,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.4167,    5.9936,    6.0985,    6.2087,    0.0000,    0.0000],\n",
      "        [-265.2083,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-132.5623,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -89.3003,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-263.8575,    1.3722,    1.4293,    1.4358,    1.4580,    1.4664],\n",
      "        [-218.6987,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -61.2514,    5.1986,    5.2900,    5.3715,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 42\n",
      "store_inventories: tensor([[-142.2629,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -91.7871,    3.2059,    3.2434,    3.2812,    0.0000,    0.0000],\n",
      "        [-170.1425,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [  -8.5924,    6.0985,    6.2087,    6.2971,    0.0000,    0.0000],\n",
      "        [-273.4838,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-136.8848,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -93.2230,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-271.3509,    1.4293,    1.4358,    1.4580,    1.4664,    1.4868],\n",
      "        [-225.0529,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -63.1922,    5.2900,    5.3715,    5.4657,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 43\n",
      "store_inventories: tensor([[-146.8991,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -91.7630,    3.2434,    3.2812,    3.3473,    0.0000,    0.0000],\n",
      "        [-174.4749,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [  -6.1081,    6.2087,    6.2971,    6.4329,    0.0000,    0.0000],\n",
      "        [-281.1001,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-139.2185,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -95.8499,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-277.4150,    1.4358,    1.4580,    1.4664,    1.4868,    1.5368],\n",
      "        [-233.3915,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -63.9914,    5.3715,    5.4657,    5.6306,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 44\n",
      "store_inventories: tensor([[-149.8930,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -94.1089,    3.2812,    3.3473,    3.3815,    0.0000,    0.0000],\n",
      "        [-178.6973,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [  -2.7789,    6.2971,    6.4329,    6.5180,    0.0000,    0.0000],\n",
      "        [-286.3841,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-142.4497,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -97.8708,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-282.5959,    1.4580,    1.4664,    1.4868,    1.5368,    1.5509],\n",
      "        [-238.1382,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -64.3975,    5.4657,    5.6306,    5.7031,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 45\n",
      "store_inventories: tensor([[-152.1036,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -95.0450,    3.3473,    3.3815,    3.4167,    0.0000,    0.0000],\n",
      "        [-182.2681,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.3692,    6.4329,    6.5180,    6.5934,    0.0000,    0.0000],\n",
      "        [-291.5241,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-145.5083,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-100.2616,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-287.0693,    1.4664,    1.4868,    1.5368,    1.5509,    1.5622],\n",
      "        [-242.7220,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -62.3884,    5.6306,    5.7031,    5.7779,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 46\n",
      "store_inventories: tensor([[-157.1091,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -97.6145,    3.3815,    3.4167,    3.4249,    0.0000,    0.0000],\n",
      "        [-187.1253,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   2.3264,    6.5180,    6.5934,    6.6336,    0.0000,    0.0000],\n",
      "        [-298.0496,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-150.4055,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-103.5181,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-293.2693,    1.4868,    1.5368,    1.5509,    1.5622,    1.5580],\n",
      "        [-248.7585,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -62.0251,    5.7031,    5.7779,    5.8159,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 47\n",
      "store_inventories: tensor([[-162.2276,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-100.4179,    3.4167,    3.4249,    3.4765,    0.0000,    0.0000],\n",
      "        [-193.3192,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   3.5851,    6.5934,    6.6336,    6.7328,    0.0000,    0.0000],\n",
      "        [-306.1680,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-156.0328,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-106.6144,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-304.5612,    1.5368,    1.5509,    1.5622,    1.5580,    1.5802],\n",
      "        [-256.2941,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -65.0999,    5.7779,    5.8159,    5.9309,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 48\n",
      "store_inventories: tensor([[-166.6942,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-101.6612,    3.4249,    3.4765,    3.5384,    0.0000,    0.0000],\n",
      "        [-197.5653,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   6.4794,    6.6336,    6.7328,    6.9073,    0.0000,    0.0000],\n",
      "        [-313.0250,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-159.2962,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-108.9347,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-309.0100,    1.5509,    1.5622,    1.5580,    1.5802,    1.6097],\n",
      "        [-262.4277,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -64.1022,    5.8159,    5.9309,    6.0771,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "t: 49\n",
      "store_inventories: tensor([[-170.2411,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-101.2018,    3.4765,    3.5384,    3.5751,    0.0000,    0.0000],\n",
      "        [-200.5210,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [  10.2651,    6.7328,    6.9073,    6.9761,    0.0000,    0.0000],\n",
      "        [-320.2347,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-161.5870,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-111.1466,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [-310.1866,    1.5622,    1.5580,    1.5802,    1.6097,    1.6227],\n",
      "        [-267.2179,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -62.3574,    5.9309,    6.0771,    6.1616,    0.0000,    0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "current_demands.shape: torch.Size([128, 10])\n",
      "store_inventories.shape: torch.Size([128, 10])\n",
      "terminated at t: 49\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # iterate over batches of train data\n",
    "    for i, data_batch in enumerate(train_loader):\n",
    "        data_batch = {k: v.to(device) for k, v in data_batch.items()}\n",
    "        observation, _ = simulator.reset(problem_params, data_batch, observation_params)\n",
    "        for t in range(problem_params['periods']):\n",
    "            print(f't: {t}')\n",
    "            print(f'store_inventories: {observation[\"store_inventories\"][0]}')\n",
    "            print()\n",
    "            # print(f'observation.keys(): {observation.keys()}')\n",
    "            action = model(observation)\n",
    "            observation, reward, terminated, _, _  = simulator.step(action)\n",
    "            if terminated:\n",
    "                print(f'terminated after t: {t}')\n",
    "                break\n",
    "        # break after first batch\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatiasRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
