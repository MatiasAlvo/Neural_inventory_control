{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from shared_imports.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from environment.ipynb\n",
      "importing Jupyter notebook from data_handling.ipynb\n",
      "importing Jupyter notebook from neural_networks.ipynb\n",
      "importing Jupyter notebook from loss_functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from shared_imports import *\n",
    "from environment import *\n",
    "from loss_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\"\n",
    "    Trainer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  device='cpu'):\n",
    "        \n",
    "        self.all_train_losses = []\n",
    "        self.all_dev_losses = []\n",
    "        self.all_test_losses = [] \n",
    "        self.device = device\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the losses\n",
    "        \"\"\"\n",
    "\n",
    "        self.all_train_losses = []\n",
    "        self.all_test_losses = []\n",
    "\n",
    "    def train(self, epochs, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in range(epochs): # make multiple passes through the dataset\n",
    "\n",
    "            average_train_loss, average_train_loss_to_report = self.do_one_epoch(\n",
    "                optimizer, \n",
    "                data_loaders['train'], \n",
    "                loss_function, \n",
    "                simulator, \n",
    "                model, \n",
    "                params_by_dataset['train']['periods'], \n",
    "                problem_params, \n",
    "                observation_params, \n",
    "                train=True, \n",
    "                ignore_periods=params_by_dataset['train']['ignore_periods']\n",
    "                )\n",
    "            \n",
    "            self.all_train_losses.append(average_train_loss_to_report)\n",
    "\n",
    "            if epoch % trainer_params['do_dev_every_n_epochs'] == 0:\n",
    "                average_dev_loss, average_dev_loss_to_report = self.do_one_epoch(\n",
    "                    optimizer, \n",
    "                    data_loaders['dev'], \n",
    "                    loss_function, \n",
    "                    simulator, \n",
    "                    model, \n",
    "                    params_by_dataset['dev']['periods'], \n",
    "                    problem_params, \n",
    "                    observation_params, \n",
    "                    train=False, \n",
    "                    ignore_periods=params_by_dataset['dev']['ignore_periods']\n",
    "                    )\n",
    "\n",
    "                self.all_dev_losses.append(average_dev_loss_to_report)\n",
    "            else:\n",
    "                average_dev_loss, average_dev_loss_to_report = 0, 0\n",
    "                self.all_dev_losses.append(self.all_dev_losses[-1])\n",
    "\n",
    "\n",
    "            # print epoch number and average per-period loss every 10 epochs\n",
    "            if epoch % trainer_params['print_results_every_n_epochs'] == 0:\n",
    "                print()\n",
    "                print(f'epoch: {epoch + 1}')\n",
    "                print(f'Average per-period train loss: {average_train_loss_to_report}')\n",
    "                print(f'Average per-period dev loss: {average_dev_loss_to_report}')\n",
    "    \n",
    "    def test(self, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, discrete_demand=False):\n",
    "\n",
    "        average_test_loss, average_test_loss_to_report = self.do_one_epoch(\n",
    "                optimizer, \n",
    "                data_loaders['test'], \n",
    "                loss_function, \n",
    "                simulator, \n",
    "                model, \n",
    "                params_by_dataset['test']['periods'], \n",
    "                problem_params, \n",
    "                observation_params, \n",
    "                train=True, \n",
    "                ignore_periods=params_by_dataset['test']['ignore_periods']\n",
    "                )\n",
    "        \n",
    "        return average_test_loss, average_test_loss_to_report\n",
    "\n",
    "    def do_one_epoch(self, optimizer, data_loader, loss_function, simulator, model, periods, problem_params, observation_params, train=True, ignore_periods=0):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_loss_to_report = 0\n",
    "        total_samples = len(data_loader.dataset)\n",
    "        periods_tracking_loss = periods - ignore_periods  # number of periods for which we report the loss\n",
    "        # print(f'total_samples: {total_samples}')\n",
    "        # print(f'periods: {periods}')\n",
    "        # print(f'ignore_periods: {ignore_periods}')\n",
    "        # print(f'periods_tracking_loss: {periods_tracking_loss}')\n",
    "        \n",
    "        for i, data_batch in enumerate(data_loader):  # loop through batches of data\n",
    "\n",
    "            data_batch = self.move_batch_to_device(data_batch)\n",
    "            \n",
    "            if train:\n",
    "                # zero-out the gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            total_reward, reward_to_report = self.simulate_batch(\n",
    "                loss_function, simulator, model, periods, problem_params, data_batch, observation_params, ignore_periods\n",
    "                )\n",
    "            epoch_loss += total_reward.item()  # rewards from period 0\n",
    "            epoch_loss_to_report += reward_to_report.item()  # rewards from period ignore_periods onwards\n",
    "            \n",
    "            mean_loss = total_reward/(len(data_batch['demands'])*periods)\n",
    "            \n",
    "            # backward pass (to calculate gradient) and take gradient step\n",
    "            if train:\n",
    "                mean_loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return epoch_loss/(total_samples*periods), epoch_loss_to_report/(total_samples*periods_tracking_loss)\n",
    "    \n",
    "    def simulate_batch(self, loss_function, simulator, model, periods, problem_params, data_batch, observation_params, ignore_periods=0):\n",
    "        \"\"\"\n",
    "        Simulate for an entire batch of data, across the specified number of periods\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize reward across batch\n",
    "        batch_reward = 0\n",
    "        reward_to_report = 0\n",
    "\n",
    "        observation, _ = simulator.reset(periods, problem_params, data_batch, observation_params)\n",
    "        for t in range(periods):\n",
    "            # print(f't: {t}')\n",
    "            # print(f'store_inventories: {observation[\"store_inventories\"].shape}')\n",
    "            \n",
    "            # print()\n",
    "            # print(f'observation.keys(): {observation.keys()}')\n",
    "            action = model(observation)\n",
    "            # action['stores'] = action['stores'].round()\n",
    "            # print(f'action: {action[\"\"][0]}')\n",
    "\n",
    "            # make a deepcopy of the past observation\n",
    "            past_observation = None\n",
    "            # past_observation = copy.deepcopy(observation)\n",
    "\n",
    "            observation, reward, terminated, _, _  = simulator.step(action)\n",
    "\n",
    "            # if t == 30:\n",
    "            #     print(f'mean reward: {reward.mean()}')\n",
    "\n",
    "            total_reward = loss_function(past_observation, action, reward)\n",
    "\n",
    "            batch_reward += total_reward\n",
    "            if t >= ignore_periods:\n",
    "                reward_to_report += total_reward\n",
    "            \n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        # return reward\n",
    "        return batch_reward, reward_to_report\n",
    "    \n",
    "    def plot_losses(self, ymin=None, ymax=None):\n",
    "        \"\"\"\n",
    "        Plot train and test losses for each epoch\n",
    "        \"\"\"\n",
    "\n",
    "        plt.plot(self.all_train_losses, label='train loss')\n",
    "        plt.plot(self.all_test_losses, label='test loss')\n",
    "        plt.legend()\n",
    "\n",
    "        if ymin is not None and ymax is not None:\n",
    "            plt.ylim(ymin, ymax)\n",
    "        plt.show()\n",
    "    \n",
    "    def move_batch_to_device(self, data_batch):\n",
    "        \"\"\"\n",
    "        Move a batch of data to the device (CPU or GPU)\n",
    "        \"\"\"\n",
    "\n",
    "        return {k: v.to(self.device) for k, v in data_batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m\n\u001b[1;32m     38\u001b[0m  store_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m      \u001b[38;5;66;03m# 'demand': {'sample_across_stores': False,\u001b[39;00m\n\u001b[1;32m     40\u001b[0m      \u001b[38;5;66;03m#                        'mean_range': [2.5, 7.5], \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m                                         }\n\u001b[1;32m     84\u001b[0m                  }\n\u001b[1;32m     87\u001b[0m  warehouse_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mholding_cost\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m, \n\u001b[1;32m     88\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlead_time\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m}\n\u001b[0;32m---> 90\u001b[0m  device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#  problem_params.update({'periods': params_by_dataset['train']['periods']})\u001b[39;00m\n\u001b[1;32m     94\u001b[0m  creator \u001b[38;5;241m=\u001b[39m DatasetCreator()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# only execute if name is main\n",
    "if __name__ == '__main__':\n",
    "    # test the class\n",
    "    # seeds = {\"underage_cost\": 28, \"holding_cost\": 73, \"mean\": 33, \"coef_of_var\": 92, \"lead_time\": 41, 'demand': 57 + 44, \"initial_inventory\": 4839}\n",
    "    seeds = {\"underage_cost\": 28, \"holding_cost\": 73, \"mean\": 33, \"coef_of_var\": 92, \"lead_time\": 41, 'demand': 57, \"initial_inventory\": 4839}\n",
    "    \n",
    "    shifted_seeds = {key: val + 8 for key, val in seeds.items()}\n",
    "    # \"seeds\": {\"underage\": 28, \"holding\": 73, \"mean\": 33, \"stds\": 92, \"demand_sequence\": 57, \"w_lead_time\": 88, \"perturbation\": 84, \"store_lead_times\": 41}\n",
    "    problem_params = {'n_stores': 1, 'n_warehouses': 0, 'n_extra_echelons': 0, 'lost_demand': False}\n",
    "    # problem_params = {'n_stores': 10, 'n_warehouses': 1, 'n_extra_echelons': 0, 'lost_demand': False}\n",
    "    params_by_dataset = {\n",
    "        'train': {'n_samples': 2**15, 'batch_size': 2**13, 'periods': 50, 'ignore_periods': 30}, \n",
    "        'dev': {'n_samples': 2**15, 'batch_size': 2**13, 'periods': 50, 'ignore_periods': 30}, \n",
    "        'test': {'n_samples': 2**15, 'batch_size': 2**13, 'periods': 500, 'ignore_periods': 300}\n",
    "        }\n",
    "    \n",
    "    trainer_params = {'do_dev_every_n_epochs': 5, 'print_results_every_n_epochs': 10}\n",
    "\n",
    "    observation_params = {\n",
    "        'include_warehouse_inventory': False,\n",
    "        'include_static_features': {\n",
    "            'holding_costs': True, \n",
    "            'underage_costs': True, \n",
    "            'lead_times': True, \n",
    "            'upper_bounds': False\n",
    "            },\n",
    "        'demand': {\n",
    "            'past_periods': 0, \n",
    "            'period_shift': 0\n",
    "            },\n",
    "        'include_past_observations': {\n",
    "            'arrivals': 0, \n",
    "            'orders': 0\n",
    "            },\n",
    "        'include_days_to_christmas': False\n",
    "        }\n",
    "\n",
    "    store_params = {\n",
    "        # 'demand': {'sample_across_stores': False,\n",
    "        #                        'mean_range': [2.5, 7.5], \n",
    "        #                     #    'mean_range': [2.5, 7.5], \n",
    "        #                        'coef_of_var_range': [0.16, 0.32],\n",
    "        #                        'distribution': 'normal',\n",
    "        #                        'correlation': 0.5,\n",
    "        #                        'clip': True\n",
    "        #                        },\n",
    "\n",
    "                      'demand': {\n",
    "                          'sample_across_stores': False,\n",
    "                          'expand': True,\n",
    "                               'mean': [5.0],\n",
    "                            #    'mean': [5.0],\n",
    "                               'std': [1.6] ,\n",
    "                               'distribution': 'normal',\n",
    "                            #    'distribution': 'poisson',\n",
    "                               'clip': True,\n",
    "                               'decimals': 3,\n",
    "                               },\n",
    "\n",
    "                    'lead_time': {'sample_across_stores': False,\n",
    "                                   'vary_across_samples': False, \n",
    "                                #    'range': [4, 7]\n",
    "                                   'expand': True,\n",
    "                                   'value': 4\n",
    "                                   },\n",
    "\n",
    "                    'holding_cost': {'sample_across_stores': False, \n",
    "                                      'vary_across_samples': False,\n",
    "                                      'expand': True,\n",
    "                                      'value': 1\n",
    "                                      },\n",
    "\n",
    "                    'underage_cost': {'sample_across_stores': False,\n",
    "                                       'vary_across_samples': False, \n",
    "                                       'expand': True, \n",
    "                                       'value': 4.0,\n",
    "                                    #    'range': [2.5, 7.5],\n",
    "                                       },\n",
    "\n",
    "                     'initial_inventory': {'sample': True,\n",
    "                                           'range_mult': [0, 1],\n",
    "                                           'inventory_periods': 4\n",
    "                                           }\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    warehouse_params = {'holding_cost': 0.3, \n",
    "                        'lead_time': 4}\n",
    "    \n",
    "\n",
    "   #  problem_params.update({'periods': params_by_dataset['train']['periods']})\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    creator = DatasetCreator()\n",
    "    \n",
    "    scenario = Scenario(\n",
    "        params_by_dataset['train']['periods'], problem_params, store_params, warehouse_params, \n",
    "        params_by_dataset['train']['n_samples'] + params_by_dataset['dev']['n_samples'], seeds\n",
    "        )\n",
    "    \n",
    "    train_dataset, dev_dataset = creator.create_datasets(scenario, split=True, by_sample_indexes=True, sample_index_for_split=params_by_dataset['dev']['n_samples'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params_by_dataset['train']['batch_size'], shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=params_by_dataset['dev']['batch_size'], shuffle=False)\n",
    "\n",
    "    scenario = Scenario(params_by_dataset['test']['periods'], problem_params, store_params, warehouse_params, params_by_dataset['test']['n_samples'], shifted_seeds)\n",
    "    test_dataset = creator.create_datasets(scenario, split=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params_by_dataset['test']['batch_size'], shuffle=False)\n",
    "\n",
    "    data_loaders = {'train': train_loader, 'dev': dev_loader, 'test': test_loader}\n",
    "    \n",
    "    neurons_per_hidden_layer = [32, 32, 32]\n",
    "    model = FullyConnectedNN(neurons_per_hidden_layer, output_size=problem_params['n_stores'], device='cpu').to(device)\n",
    "    loss_function = PolicyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    simulator = Simulator(device=device)\n",
    "    trainer = Trainer(device=device)\n",
    "    trainer.train(1000, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1\n",
      "Average per-period train loss: 306.3996780395508\n",
      "Average per-period dev loss: 41.67693176269531\n",
      "\n",
      "epoch: 11\n",
      "Average per-period train loss: 6.318474388122558\n",
      "Average per-period dev loss: 6.793968391418457\n",
      "\n",
      "epoch: 21\n",
      "Average per-period train loss: 5.594799423217774\n",
      "Average per-period dev loss: 5.585102367401123\n",
      "\n",
      "epoch: 31\n",
      "Average per-period train loss: 5.366036128997803\n",
      "Average per-period dev loss: 5.371010780334473\n",
      "\n",
      "epoch: 41\n",
      "Average per-period train loss: 5.247053432464599\n",
      "Average per-period dev loss: 5.259267044067383\n",
      "\n",
      "epoch: 51\n",
      "Average per-period train loss: 5.191890716552734\n",
      "Average per-period dev loss: 5.205890560150147\n",
      "\n",
      "epoch: 61\n",
      "Average per-period train loss: 5.149415397644043\n",
      "Average per-period dev loss: 5.164308738708496\n",
      "\n",
      "epoch: 71\n",
      "Average per-period train loss: 5.109719562530517\n",
      "Average per-period dev loss: 5.124061584472656\n",
      "\n",
      "epoch: 81\n",
      "Average per-period train loss: 5.07252254486084\n",
      "Average per-period dev loss: 5.0866899490356445\n",
      "\n",
      "epoch: 91\n",
      "Average per-period train loss: 5.046859169006348\n",
      "Average per-period dev loss: 5.062322521209717\n",
      "\n",
      "epoch: 101\n",
      "Average per-period train loss: 5.035739326477051\n",
      "Average per-period dev loss: 5.052218818664551\n",
      "\n",
      "epoch: 111\n",
      "Average per-period train loss: 5.029308319091797\n",
      "Average per-period dev loss: 5.044935894012451\n",
      "\n",
      "epoch: 121\n",
      "Average per-period train loss: 5.023967552185058\n",
      "Average per-period dev loss: 5.04031810760498\n",
      "\n",
      "epoch: 131\n",
      "Average per-period train loss: 5.0213698387146\n",
      "Average per-period dev loss: 5.037765312194824\n",
      "\n",
      "epoch: 141\n",
      "Average per-period train loss: 5.019822788238526\n",
      "Average per-period dev loss: 5.036278152465821\n",
      "\n",
      "epoch: 151\n",
      "Average per-period train loss: 5.01854887008667\n",
      "Average per-period dev loss: 5.035476112365723\n",
      "\n",
      "epoch: 161\n",
      "Average per-period train loss: 5.017780017852783\n",
      "Average per-period dev loss: 5.035277843475342\n",
      "\n",
      "epoch: 171\n",
      "Average per-period train loss: 5.017409801483154\n",
      "Average per-period dev loss: 5.033561420440674\n",
      "\n",
      "epoch: 181\n",
      "Average per-period train loss: 5.016925430297851\n",
      "Average per-period dev loss: 5.033281707763672\n",
      "\n",
      "epoch: 191\n",
      "Average per-period train loss: 5.016583061218261\n",
      "Average per-period dev loss: 5.033096408843994\n",
      "\n",
      "epoch: 201\n",
      "Average per-period train loss: 5.0171998023986815\n",
      "Average per-period dev loss: 5.035233402252198\n",
      "\n",
      "epoch: 211\n",
      "Average per-period train loss: 5.016339683532715\n",
      "Average per-period dev loss: 5.033094882965088\n",
      "\n",
      "epoch: 221\n",
      "Average per-period train loss: 5.015953159332275\n",
      "Average per-period dev loss: 5.032173728942871\n",
      "\n",
      "epoch: 231\n",
      "Average per-period train loss: 5.016266632080078\n",
      "Average per-period dev loss: 5.032109260559082\n",
      "\n",
      "epoch: 241\n",
      "Average per-period train loss: 5.015855979919434\n",
      "Average per-period dev loss: 5.031785106658935\n",
      "\n",
      "epoch: 251\n",
      "Average per-period train loss: 5.016549777984619\n",
      "Average per-period dev loss: 5.033326053619385\n",
      "\n",
      "epoch: 261\n",
      "Average per-period train loss: 5.015984153747558\n",
      "Average per-period dev loss: 5.031962108612061\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mTrain the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs): \u001b[38;5;66;03m# make multiple passes through the dataset\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     average_train_loss, average_train_loss_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperiods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_periods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_periods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_train_losses\u001b[38;5;241m.\u001b[39mappend(average_train_loss_to_report)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m trainer_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_dev_every_n_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 82\u001b[0m, in \u001b[0;36mTrainer.do_one_epoch\u001b[0;34m(self, optimizer, data_loader, loss_function, simulator, model, periods, problem_params, observation_params, train, ignore_periods)\u001b[0m\n\u001b[1;32m     76\u001b[0m periods_tracking_loss \u001b[38;5;241m=\u001b[39m periods \u001b[38;5;241m-\u001b[39m ignore_periods  \u001b[38;5;66;03m# number of periods for which we report the loss\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# print(f'total_samples: {total_samples}')\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# print(f'periods: {periods}')\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# print(f'ignore_periods: {ignore_periods}')\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# print(f'periods_tracking_loss: {periods_tracking_loss}')\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):  \u001b[38;5;66;03m# loop through batches of data\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     data_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove_batch_to_device(data_batch)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# zero-out the gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/MatiasRL/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/MatiasRL/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.conda/envs/MatiasRL/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/MatiasRL/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m<string>:11\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "File \u001b[0;32m<string>:11\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatiasRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
